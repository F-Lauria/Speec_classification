{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import save\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load  processed data\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle = True)\n",
    "labels = np.load(\"labels.npy\", allow_pickle = True)\n",
    "training_data.shape,training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althoug, we can reach a good accuracy with 2D layers, the 1D layer turned out to have better accuracy for this data. This is not surprusing, 1D CNN it is widely used for speech recognition. So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data, reshape and transform the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go back to two three dimensions\n",
    "train_X = training_data\n",
    "#test_X = test_data\n",
    "train_Y = labels\n",
    "\n",
    "train_X = train_X.reshape(-1, 99,13)\n",
    "#test_X = test_X.reshape(-1, 99,13)\n",
    "\n",
    "# transform the labels\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "\n",
    "train_X.shape #, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE THE VALIDATION SET \n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, \n",
    "                                                           random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters \n",
    "batch_size = 256\n",
    "epochs = 1\n",
    "num_classes = 35 # fix\n",
    "np.random.seed(222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a very deep model. My group and I spent a lot of time tuning this model because we knew we could reach a very high accuracy. Eventually we were able to reach 95% but I know that it is also possible to get 98%. I leave you this challenge to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model = Sequential()\n",
    "\n",
    "fashion_model.add(Conv1D(64, kernel_size=6,activation='relu',padding='same',input_shape=(99,13)))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "\n",
    "fashion_model.add(Dropout(0.2))\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(256, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(256, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(256, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(512, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(1024, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(1024, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(1024, activation='relu'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))           \n",
    "fashion_model.add(Dropout(0.2))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                      optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_train = fashion_model.fit(train_X, train_label, \n",
    "                                  batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                                  validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I remember correctly, we trained our model for 100 epochs. As always here, for the sake of the example I just ran it for 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have discussed three different ways of dealing with a speech classification task. First, we tried to solve the problem with a ML algorithm, however, we could not go over 20% accuracy, which is normal becuase this kinds of tasks require to apply DL methods. Therefore, two different types of CNN layers were applied: 2D and 1D. As a result a very high accuracy was reached, altoghou a higher one is possible to achive.\n",
    "\n",
    "What do we bring home?\n",
    "- These kinds of tasks cannot be solve with classic ML algortihms\n",
    "- A lot of time is recquired for tuning DL models and unfortunately we do not know which one is the best. Try and error is often the standard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
