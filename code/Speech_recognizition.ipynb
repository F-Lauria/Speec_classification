{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech recognition task\n",
    "In this notebook I will guide you in one of the most common challenge of these days, that is, a speech recognition task. This is a **classification** task, our dataset is composed of 105835 .wav recordings with 35 possible labels and for each recording we have Mel-frequencies. Firstly, we will see how to load data and how to process them. Secondly, we will use differnt algorithms to predict our classes. Particularly, we will deal with:\n",
    "\n",
    "1. **Decision forest**\n",
    "2. **CNN 2D**\n",
    "3. **CNN 1D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "The first thing to do is to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to load the data and then to precess them. Before starting, let's see in more details our data.\n",
    "you can find data here: https://surfdrive.surf.nl/files/index.php/s/A91xgk7B5kXNvfJ\n",
    "\n",
    "- **feat.npy** is an array with Mel-frequency cepstral coefficients extracted from each wav file. The features at index *i* in this array were extracted from the wav file at index *i* of the array in the file path.npy.\n",
    "- **path.npy** is an array with the order of wav files in the feat.npy array.\n",
    "- **train.csv** contains two columns: path with the filename of the recording and word with word which was pronounced in the recording. This is the training portion of the data.\n",
    "- **test.csv** is the testing portion of the data, and it has the same format as the file train.csv except that the column word is absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # --------------------- LOAD DATA ------------------------ #\n",
    "\n",
    "features = np.load(\"feat.npy\", allow_pickle = True)\n",
    "path = np.load(\"path.npy\", allow_pickle = True)\n",
    "train = pd.read_csv(\"train.csv\", delimiter = \",\")\n",
    "test = pd.read_csv(\"test.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu put it simply, in path.npy we have the \"wav name\" of the recordings and in feat.npy we have its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The file with name \",path[0], \"has this set of features:\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the index in path and features must be the same, otherwise we are going to misalign \"wav file name - features\" and consequently, have a very bad accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now stay with me, because this is the most boring part of the project. Unfortunatly the data are not set up to work directly with them and we cannot apply ML or DL algortihms (yet), but this is fine, we are Data Scientists and we know that we have to get \"our hands dirty\" before having fun ;).\n",
    "\n",
    "The problem is that we already have the split of train set and test set (see train.csv and test.csv). I know, now you are thinking: \"Why in the world is this a problem? we need those!\". That is true, but the problem is that we just have the \"file name\" already splitted and not also their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.head())\n",
    "print()\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we need to find a way to split our data, which are divided in path.npy and feat.npy, according to the indices in train.csv and test.csv.\n",
    "\n",
    "To do that, we first link our path.npy and feat.npy with a dictionary, key = path and value = feat. You will understand why in a moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # ------------------- DATA PROCESSING ---------------------- #\n",
    "\n",
    "# create dictionary: key = path and value = feat\n",
    "dic = {} \n",
    "for i in range(len(path)):\n",
    "    dic[path[i]] = features[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a dictionary, we need a fuction that allows us to split all our data (now in the dictionary) into train and test sets according to the csv files. Basically, we go through all the data_frame, which are train.csv and test.csv and thanks to the information into the dictionary we can create a list for each data_frame, where in the *ith* position for both, data_frame and dictionary, we have its *i* set of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function take as argument a pandas data frame and a dictionary\n",
    "# and create a new list according to the ith position in the dataframe\n",
    "\n",
    "def create_list(data_frame,dic):\n",
    "    new_list= []\n",
    "    for i in range(len(data_frame)):\n",
    "        if data_frame[\"path\"][i] in dic.keys():\n",
    "            new_list.append(dic[data_frame[\"path\"][i]]) # in the position i we add its features thanks to the dic\n",
    "    return new_list\n",
    "\n",
    "#in order to convert a list in a numpy array we need to padd our data\n",
    "def padding(data):\n",
    "    zeros_list=[0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for example in range(len(data)):\n",
    "        if data[example].shape[0]!=99:\n",
    "            to_change=data[example].tolist()\n",
    "            for adding in range(99-len(to_change)):\n",
    "                to_change.append(zeros_list)\n",
    "            data[example]=np.array(to_change)     \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply all the defined functions and we can also have an overview of the shape so that we have a feeling about what kind of data we are going to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94824, 99, 13), (11005, 99, 13))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split test and train \n",
    "training_data = create_list(train,dic)\n",
    "test_data = create_list(test,dic)\n",
    "\n",
    "# padding\n",
    "training_data = padding(training_data)\n",
    "test_data = padding(test_data)\n",
    "\n",
    "# convert to array\n",
    "training_data = np.array(training_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "#check shape\n",
    "training_data.shape,test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it, we have now a ordered training data and test data, respectively compose of 94824 and 11005 examples with 99 lists of 13 elements each. The new way or linkig all the features that we have now in the variables training_data and test_data is with the train.csv and test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This file name\", train.loc[0], \"\\n\\n has this features \\n\" )\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start applying some model, but first we need to encode our target. We will use the function [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html), because our target is a string and it is much easier to work with integer and so we encode our labels into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # ------------- set up data for running DECISION FOREST ----------------#\n",
    "\n",
    "# encode the target\n",
    "train_numpy = train.values\n",
    "labels = train_numpy[:,1]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "labels = encoder.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create now a validation data for testing the decision forest thanks to the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLITTING\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75859, 99, 13)\n",
      "(75859,)\n",
      "(18965, 99, 13)\n",
      "(18965,)\n"
     ]
    }
   ],
   "source": [
    "# check shape\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the accuracy of our decision forest, it is wise to do first some features engineering. We define two functions. The first creates simply the mean of our signal, while the second does something a little bit more complicated, that is, it applys different statistics indices to our signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    # ---------- features engineering for decision forest  -------#\n",
    "\n",
    "def features_mean(signal):\n",
    "    return np.mean(signal,axis=2)\n",
    "\n",
    "def features(signal, functions):\n",
    "    summaries=[]\n",
    "    for fn in functions:\n",
    "        summaries.append(fn(signal,axis=2))\n",
    "    return np.concatenate(summaries,axis=1)\n",
    "\n",
    "summaries = [np.mean, np.min, np.max, np.std]\n",
    "\n",
    "X_train_summaries = features(X_train, summaries)\n",
    "X_test_summaries = features(X_test, summaries)\n",
    "\n",
    "X_train_mean = features_mean(X_train)\n",
    "X_test_mean = features_mean(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit of fun has arrived. Here I create a function in order to run the decision forest with different number of nodes. As you know, the number of nodes in a forest is extremely important as it allows to control for overfitting. Let's have a look at all the arguments:\n",
    "\n",
    "- **n_estimators** : this is a list and as I anticipate, with this we can control the number of nodes\n",
    "- **X_traind and X_test** : because we used 2 different kinds of features engineering, here we can choose which one to use\n",
    "- **y_train and y_test** : regardless wich kind of training data we use, we will always use the same test set and for this reason these arguments have already default values\n",
    "- **random_state** : simply a way of controlling for randomness when looking for the best split at each node. This is usefull in order to be able to reproduce the accuracy.\n",
    "\n",
    "I leave you [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) the official documentation about decision forest.\n",
    "\n",
    "In conclusion, not only this function allows us to choose the training data and the number of nodes, but it saves for each number of nodes its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function take as argument a training and a validation data set and return the accuracy based on \n",
    "# the number of nodes, which is encode here as n_estimators\n",
    "\n",
    "def run_forest(n_estimators ,X_train, X_test, y_train = y_train, y_test = y_test , random_state = 333):\n",
    "    \n",
    "    acc=[] # list of accuracy which depends on the hyperparameter n_estimators\n",
    "\n",
    "    for num_features in n_estimators: # hyperparameters to change with higher number\n",
    "        forest = RandomForestClassifier(n_estimators=num_features, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt',\n",
    "                                random_state = random_state)\n",
    "        fitted_model=forest.fit(X_train,y_train)\n",
    "        prediction=fitted_model.predict(X_test)\n",
    "        accuracy=accuracy_score(y_test,prediction)\n",
    "        solution = (num_features,accuracy)\n",
    "        acc.append(solution)\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model, here we can define the lists to put as **n_estimators** argument. My machine cannot handle all these data, for the sake of the example, I just put 1 node for each type of features engineering. You can try with [50,100,200]  and you will see that the accuracy is going to increase, although it cannot go over 20%. Not too much eh? For this we have DL ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up different number of nodes \n",
    "n_estimators_mean = [1]\n",
    "n_estimators_summaries = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.05715792248879515)]\n"
     ]
    }
   ],
   "source": [
    "# accuracy for the mean\n",
    "accuracy_mean = run_forest(n_estimators_mean,X_train_mean,X_test_mean)\n",
    "print(accuracy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.07835486422356973)]\n"
     ]
    }
   ],
   "source": [
    "# accuracy for the summaries\n",
    "accuracy_summaries = run_forest(n_estimators_summaries,X_train_summaries,X_test_summaries)\n",
    "print(accuracy_summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean everything by uploading in new variables our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name of variables \n",
    "train_X = training_data\n",
    "test_X = test_data\n",
    "train_Y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the Conv2D function, which is mainly use for image classification, but also in this case we can reach an important accuracy (more than 90%). Because of that, we need to reshape our data by adding a fictitious size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94824, 99, 13, 1), (11005, 99, 13, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape in four dimensions for input CNN\n",
    "train_X = train_X.reshape(-1, 99,13, 1)\n",
    "test_X = test_X.reshape(-1, 99,13, 1)\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data type in float\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With CNN we are going to predict a probability for each possible class for each example and for this we need to change again our target. We will use the [to_categorical](https://keras.io/utils/) function. This function creates for each target a list with as many position as number of classes-1 (because it starts from 0) and each *ith* position represents the *ith* class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the labels\n",
    "train_Y_one_hot = to_categorical(train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here we can see that the first target is the 22° class, this is because in the 21th position we have 1, while in the others we have 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_one_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE THE VALIDATION SET \n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, \n",
    "                                                           random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75859, 99, 13, 1), (18965, 99, 13, 1), (75859, 35), (18965, 35))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all the shape\n",
    "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might know, before running CNN we need to set up some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #----------------------------------- first attempt ---------------------------------#\n",
    "\n",
    "# set up hyperparameters \n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "num_classes = 35 # fix\n",
    "np.random.seed(222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our model: 3 CNN2D layers and a fully connected layer before the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up the layers \n",
    "fashion_model = Sequential()\n",
    "\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(99,13,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))  \n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                      optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 99, 13, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 99, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 7, 64)         18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 50, 7, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 4, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 4, 128)        73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 25, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 2, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               426112    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 35)                4515      \n",
      "=================================================================\n",
      "Total params: 523,299\n",
      "Trainable params: 523,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## check the summary\n",
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run. Also here, I just use one epoch for the sake of the example. You should try with at least 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 75859 samples, validate on 18965 samples\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "75859/75859 [==============================] - 260s 3ms/step - loss: 1.2988 - acc: 0.6328 - val_loss: 0.6284 - val_acc: 0.8168\n"
     ]
    }
   ],
   "source": [
    "## train and test the accuracy in the validation set\n",
    "fashion_train = fashion_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,\n",
    "                                  verbose=1,validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, with this model you should reach 85% accuracy in validation. Not bad eh? However, we can go much more further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to control overfitting by introducing the function and we hope to improve accuracy. Spoiler alert: we did!\n",
    "\n",
    "To put it simply, during training, some number of layer outputs are randomly ignored or \"dropped out.\" For this we use less parameters and as a result we can control overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters \n",
    "batch_size = 124\n",
    "epochs = 1\n",
    "num_classes = 35 # fix\n",
    "np.random.seed(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/francescolauria/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "## set up the dropout to improve accuracy\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(99,13,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.25))\n",
    "\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.25))\n",
    "\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.4))\n",
    "\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))  \n",
    "fashion_model.add(Dropout(0.3))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                      optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 99, 13, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 99, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 50, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 7, 64)         18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 50, 7, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 25, 4, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 4, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 4, 128)        73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 25, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 2, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 13, 2, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               426112    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 35)                4515      \n",
      "=================================================================\n",
      "Total params: 523,299\n",
      "Trainable params: 523,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75859 samples, validate on 18965 samples\n",
      "Epoch 1/1\n",
      "75859/75859 [==============================] - 366s 5ms/step - loss: 2.4425 - acc: 0.3238 - val_loss: 1.2589 - val_acc: 0.6495\n"
     ]
    }
   ],
   "source": [
    "fashion_train = fashion_model.fit(train_X, train_label, \n",
    "                                  batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                                  validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, with this model you should reach at least 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94824, 99, 13), (11005, 99, 13))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#go back to two three dimensions\n",
    "train_X = training_data\n",
    "test_X = test_data\n",
    "train_Y = labels\n",
    "\n",
    "train_X = train_X.reshape(-1, 99,13)\n",
    "test_X = test_X.reshape(-1, 99,13)\n",
    "\n",
    "# transform the labels\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE THE VALIDATION SET \n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, \n",
    "                                                           random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters \n",
    "batch_size = 256\n",
    "epochs = 1\n",
    "num_classes = 35 # fix\n",
    "np.random.seed(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model = Sequential()\n",
    "\n",
    "fashion_model.add(Conv1D(64, kernel_size=6,activation='relu',padding='same',input_shape=(99,13)))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "\n",
    "fashion_model.add(Dropout(0.2))\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(256, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(256, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(256, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(512, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(1024, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(1024, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(1024, activation='relu'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))           \n",
    "fashion_model.add(Dropout(0.2))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 99, 64)            5056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 99, 64)            256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 99, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 50, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 128)           512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 50, 128)           98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 128)           512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 25, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 25, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 25, 128)           98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25, 128)           512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 25, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 13, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 13, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 13, 128)           98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 13, 128)           512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 13, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 4, 256)            393472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 256)            1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 2, 256)            393472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 2, 256)            1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1, 512)            2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1, 1024)           3146752   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 1, 1024)           6292480   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 35)                35875     \n",
      "=================================================================\n",
      "Total params: 12,660,707\n",
      "Trainable params: 12,652,899\n",
      "Non-trainable params: 7,808\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                      optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75859 samples, validate on 18965 samples\n",
      "Epoch 1/1\n",
      "75859/75859 [==============================] - 1961s 26ms/step - loss: 2.5409 - acc: 0.2798 - val_loss: 1.8575 - val_acc: 0.4542\n"
     ]
    }
   ],
   "source": [
    "fashion_train = fashion_model.fit(train_X, train_label, \n",
    "                                  batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                                  validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # ----------------------- PREDICTION ON TEST SET ---------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['backward' 'yes' 'seven' ... 'five' 'backward' 'backward']\n"
     ]
    }
   ],
   "source": [
    "#### make prediction on the test set\n",
    "test_prediction = fashion_model.predict(test_X) # take the prediction from the model\n",
    "test_prediction = np.argmax(np.round(test_prediction),axis=1) # take the index of the maximum probability\n",
    "test_prediction = encoder.inverse_transform(test_prediction) # from number label to word\n",
    "print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the column of prediction to the test csv\n",
    "test_csv = pd.read_csv(\"test.csv\", delimiter = \",\",index_col=None)\n",
    "test_csv['word'] = test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the csv\n",
    "out_csv = \"result.csv\"\n",
    "test_csv.to_csv(out_csv,index=None,header=True)\n",
    "#export_csv = test_csv.to_csv (\"~/Desktop/Tilburg_university/Block2/export_dataframe.csv\", \n",
    "#                              index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
