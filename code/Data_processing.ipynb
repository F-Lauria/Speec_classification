{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "The first thing to do is to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to load the data and then to precess them. Before starting, let's see in more details our data.\n",
    "you can find data here: https://surfdrive.surf.nl/files/index.php/s/A91xgk7B5kXNvfJ\n",
    "\n",
    "- **feat.npy** is an array with Mel-frequency cepstral coefficients extracted from each wav file. The features at index *i* in this array were extracted from the wav file at index *i* of the array in the file path.npy.\n",
    "- **path.npy** is an array with the order of wav files in the feat.npy array.\n",
    "- **train.csv** contains two columns: path with the filename of the recording and word with word which was pronounced in the recording. This is the training portion of the data.\n",
    "- **test.csv** is the testing portion of the data, and it has the same format as the file train.csv except that the column word is absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # --------------------- LOAD DATA ------------------------ #\n",
    "\n",
    "features = np.load(\"feat.npy\", allow_pickle = True)\n",
    "path = np.load(\"path.npy\", allow_pickle = True)\n",
    "train = pd.read_csv(\"train.csv\", delimiter = \",\")\n",
    "test = pd.read_csv(\"test.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu put it simply, in path.npy we have the \"wav name\" of the recordings and in feat.npy we have its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The file with name \",path[0], \"has this set of features:\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the index in path and features must be the same, otherwise we are going to misalign \"wav file name - features\" and consequently, have a very bad accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now stay with me, because this is the most boring part of the project. Unfortunatly the data are not set up to work directly with them and we cannot apply ML or DL algortihms (yet), but this is fine, we are Data Scientists and we know that we have to get \"our hands dirty\" before having fun ;).\n",
    "\n",
    "The problem is that we already have the split of train set and test set (see train.csv and test.csv). I know, now you are thinking: \"Why in the world is this a problem? we need those!\". That is true, but the problem is that we just have the \"file name\" already splitted and not also their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.head())\n",
    "print()\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we need to find a way to split our data, which are divided in path.npy and feat.npy, according to the indices in train.csv and test.csv.\n",
    "\n",
    "To do that, we first link our path.npy and feat.npy with a dictionary, key = path and value = feat. You will understand why in a moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary: key = path and value = feat\n",
    "dic = {} \n",
    "for i in range(len(path)):\n",
    "    dic[path[i]] = features[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a dictionary, we need a fuction that allows us to split all our data (now in the dictionary) into train and test sets according to the csv files. Basically, we go through all the data_frame, which are train.csv and test.csv and thanks to the information into the dictionary we can create a list for each data_frame, where in the *ith* position for both, data_frame and dictionary, we have its *i* set of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function take as argument a pandas data frame and a dictionary\n",
    "# and create a new list according to the ith position in the dataframe\n",
    "\n",
    "def create_list(data_frame,dic):\n",
    "    new_list= []\n",
    "    for i in range(len(data_frame)):\n",
    "        if data_frame[\"path\"][i] in dic.keys():\n",
    "            new_list.append(dic[data_frame[\"path\"][i]]) # in the position i we add its features thanks to the dic\n",
    "    return new_list\n",
    "\n",
    "#in order to convert a list in a numpy array we need to padd our data\n",
    "def padding(data):\n",
    "    zeros_list=[0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for example in range(len(data)):\n",
    "        if data[example].shape[0]!=99:\n",
    "            to_change=data[example].tolist()\n",
    "            for adding in range(99-len(to_change)):\n",
    "                to_change.append(zeros_list)\n",
    "            data[example]=np.array(to_change)     \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply all the defined functions and we can also have an overview of the shape so that we have a feeling about what kind of data we are going to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and train \n",
    "training_data = create_list(train,dic)\n",
    "test_data = create_list(test,dic)\n",
    "\n",
    "# padding\n",
    "training_data = padding(training_data)\n",
    "test_data = padding(test_data)\n",
    "\n",
    "# convert to array\n",
    "training_data = np.array(training_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "#check shape\n",
    "training_data.shape,test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it, we have now a ordered training data and test data, respectively compose of 94824 and 11005 examples with 99 lists of 13 elements each. The new way or linkig all the features that we have now in the variables training_data and test_data is with the train.csv and test.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
