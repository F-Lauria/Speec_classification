{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech recognition task\n",
    "In this notebook I will guide you in one of the most common challenge of these days, that is, a speech recognition task. This is a **classification** task, our dataset is composed of 105835 .wav recordings with 35 possible labels and for each recording we have Mel-frequencies. Firstly, we will see how to load data and how to process them. Secondly, we will use differnt algorithms to predict our classes. Particularly, we will deal with:\n",
    "\n",
    "1. **Decision forest**\n",
    "2. **CNN 2D**\n",
    "3. **CNN 1D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "The first thing to do is to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import save\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to load the data and then to precess them. Before starting, let's see in more details our data.\n",
    "you can find data here: https://surfdrive.surf.nl/files/index.php/s/A91xgk7B5kXNvfJ\n",
    "\n",
    "- **feat.npy** is an array with Mel-frequency cepstral coefficients extracted from each wav file. The features at index *i* in this array were extracted from the wav file at index *i* of the array in the file path.npy.\n",
    "- **path.npy** is an array with the order of wav files in the feat.npy array.\n",
    "- **train.csv** contains two columns: path with the filename of the recording and word with word which was pronounced in the recording. This is the training portion of the data.\n",
    "- **test.csv** is the testing portion of the data, and it has the same format as the file train.csv except that the column word is absent.\n",
    "\n",
    "For transparency reason I also described the test portion of the data. However, I do not have access to its label so for now on I will not consider it. We will divide our data in training and validation set, this is not a problem because we have enough data for doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # --------------------- LOAD DATA ------------------------ #\n",
    "\n",
    "features = np.load(\"feat.npy\", allow_pickle = True)\n",
    "path = np.load(\"path.npy\", allow_pickle = True)\n",
    "train = pd.read_csv(\"train.csv\", delimiter = \",\")\n",
    "#test = pd.read_csv(\"test.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu put it simply, in path.npy we have the \"wav name\" of the recordings and in feat.npy we have its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The file with name \",path[0], \"has this set of features:\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the index in path and features must be the same, otherwise we are going to misalign \"wav file name - features\" and consequently, have a very bad accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now stay with me, because this is the most boring part of the project. Unfortunatly the data are not set up to work directly with them and we cannot apply ML or DL algortihms (yet), but this is fine, we are Data Scientists and we know that we have to get \"our hands dirty\" before having fun ;).\n",
    "\n",
    "The problem is that we already have the split of train set and test set (see train.csv and test.csv). I know, now you are thinking: \"Why in the world is this a problem? we need those!\". That is true, but the problem is that we just have the \"file name\" already splitted and not also their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test.head())\n",
    "print()\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we need to find a way to split our data, which are divided in path.npy and feat.npy, according to the indices in train.csv and test.csv.\n",
    "\n",
    "To do that, we first link our path.npy and feat.npy with a dictionary, key = path and value = feat. You will understand why in a moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary: key = path and value = feat\n",
    "dic = {} \n",
    "for i in range(len(path)):\n",
    "    dic[path[i]] = features[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a dictionary, we need a fuction that allows us to split all our data (now in the dictionary) into train and test sets according to the csv files. Basically, we go through all the data_frame, which are train.csv and test.csv and thanks to the information into the dictionary we can create a list for each data_frame, where in the *ith* position for both, data_frame and dictionary, we have its *i* set of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function take as argument a pandas data frame and a dictionary\n",
    "# and create a new list according to the ith position in the dataframe\n",
    "\n",
    "def create_list(data_frame,dic):\n",
    "    new_list= []\n",
    "    for i in range(len(data_frame)):\n",
    "        if data_frame[\"path\"][i] in dic.keys():\n",
    "            new_list.append(dic[data_frame[\"path\"][i]]) # in the position i we add its features thanks to the dic\n",
    "    return new_list\n",
    "\n",
    "#in order to convert a list in a numpy array we need to padd our data\n",
    "def padding(data):\n",
    "    zeros_list=[0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for example in range(len(data)):\n",
    "        if data[example].shape[0]!=99:\n",
    "            to_change=data[example].tolist()\n",
    "            for adding in range(99-len(to_change)):\n",
    "                to_change.append(zeros_list)\n",
    "            data[example]=np.array(to_change)     \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply all the defined functions and we can also have an overview of the shape so that we have a feeling about what kind of data we are going to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and train \n",
    "training_data = create_list(train,dic)\n",
    "#test_data = create_list(test,dic)\n",
    "\n",
    "# padding\n",
    "training_data = padding(training_data)\n",
    "#test_data = padding(test_data)\n",
    "\n",
    "# convert to array\n",
    "training_data = np.array(training_data)\n",
    "#test_data = np.array(test_data)\n",
    "\n",
    "#check shape\n",
    "training_data.shape #,test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it, we have now a ordered training data and test data, respectively compose of 94824 and 11005 examples with 99 lists of 13 elements each. The new way or linkig all the features that we have now in the variables training_data and test_data is with the train.csv and test.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This file name\", train.loc[0], \"\\n\\n has this features \\n\" )\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Almost finish with this chapter, but first we need to encode our target. We will use the function [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html), because our target is a string and it is much easier to work with integer and so we encode our labels into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the target\n",
    "train_numpy = train.values\n",
    "labels = train_numpy[:,1]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "labels = encoder.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid always doing this data processing let's save these new data so that you can use it for running the future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE PROCESSED DATA\n",
    "save('training_data.npy', training_data)\n",
    "save(\"labels.npy\",labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create now a validation data for testing the decision forest thanks to the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLITTING\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the accuracy of our decision forest, it is wise to do first some features engineering. We define two functions. The first creates simply the mean of our signal, while the second does something a little bit more complicated, that is, it applys different statistics indices to our signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_mean(signal):\n",
    "    return np.mean(signal,axis=2)\n",
    "\n",
    "def features(signal, functions):\n",
    "    summaries=[]\n",
    "    for fn in functions:\n",
    "        summaries.append(fn(signal,axis=2))\n",
    "    return np.concatenate(summaries,axis=1)\n",
    "\n",
    "summaries = [np.mean, np.min, np.max, np.std]\n",
    "\n",
    "X_train_summaries = features(X_train, summaries)\n",
    "X_test_summaries = features(X_test, summaries)\n",
    "\n",
    "X_train_mean = features_mean(X_train)\n",
    "X_test_mean = features_mean(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit of fun has arrived. Here I create a function in order to run the decision forest with different number of nodes. As you know, the number of nodes in a forest is extremely important as it allows to control for overfitting. Let's have a look at all the arguments:\n",
    "\n",
    "- **n_estimators** : this is a list and as I anticipate, with this we can control the number of nodes\n",
    "- **X_traind and X_test** : because we used 2 different kinds of features engineering, here we can choose which one to use\n",
    "- **y_train and y_test** : regardless wich kind of training data we use, we will always use the same test set and for this reason these arguments have already default values\n",
    "- **random_state** : simply a way of controlling for randomness when looking for the best split at each node. This is usefull in order to be able to reproduce the accuracy.\n",
    "\n",
    "I leave you [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) the official documentation about decision forest.\n",
    "\n",
    "In conclusion, not only this function allows us to choose the training data and the number of nodes, but it saves for each number of nodes its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function take as argument a training and a validation data set and return the accuracy based on \n",
    "# the number of nodes, which is encode here as n_estimators\n",
    "\n",
    "def run_forest(n_estimators ,X_train, X_test, y_train = y_train, y_test = y_test , random_state = 333):\n",
    "    \n",
    "    acc=[] # list of accuracy which depends on the hyperparameter n_estimators\n",
    "\n",
    "    for num_features in n_estimators: # hyperparameters to change with higher number\n",
    "        forest = RandomForestClassifier(n_estimators=num_features, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt',\n",
    "                                random_state = random_state)\n",
    "        fitted_model=forest.fit(X_train,y_train)\n",
    "        prediction=fitted_model.predict(X_test)\n",
    "        accuracy=accuracy_score(y_test,prediction)\n",
    "        solution = (num_features,accuracy)\n",
    "        acc.append(solution)\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model, here we can define the lists to put as **n_estimators** argument. My machine cannot handle all these data, for the sake of the example, I just put 1 node for each type of features engineering. You can try with [50,100,200]  and you will see that the accuracy is going to increase, although it cannot go over 20%. Not too much eh? For this we have DL ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up different number of nodes \n",
    "n_estimators_mean = [1]\n",
    "n_estimators_summaries = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy for the mean\n",
    "accuracy_mean = run_forest(n_estimators_mean,X_train_mean,X_test_mean)\n",
    "print(accuracy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy for the summaries\n",
    "accuracy_summaries = run_forest(n_estimators_summaries,X_train_summaries,X_test_summaries)\n",
    "print(accuracy_summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now the moment that all of us were waiting, let's apply Convolutional Neural Network. In this case, we will apply the 2D layer, which is mainly use for image classification, but also in this case we can reach an important accuracy (more than 90%) by reshaping the dimensions of our input shape. For creating this model, I was inspired by this very well done [tutorial](https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python) on DataCamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's upload the data in new variables so that we have everything nice and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name of variables \n",
    "train_X = training_data\n",
    "#test_X = test_data\n",
    "train_Y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said before, we need to reshape the input space, [reshape function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) can help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape in four dimensions for input CNN\n",
    "train_X = train_X.reshape(-1, 99,13, 1)\n",
    "#test_X = test_X.reshape(-1, 99,13, 1)\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data type in float\n",
    "train_X = train_X.astype('float32')\n",
    "#test_X = test_X.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With CNN we are going to predict a probability for each possible class for each example and for this we need to change again our target. We will use the [to_categorical](https://keras.io/utils/) function. This function creates for each target a list with as many position as number of classes-1 (because it starts from 0) and each *ith* position represents the *ith* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the labels\n",
    "train_Y_one_hot = to_categorical(train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here we can see that the first target is the 22Â° class, this is because in the 21th position we have 1, while in the others we have 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_one_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the model we need of course to create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE THE VALIDATION SET \n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, \n",
    "                                                           random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all the shape\n",
    "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might know, before running CNN we need to set up some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters \n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "num_classes = 35 # fix\n",
    "np.random.seed(222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our model: 3 CNN2D layers and a fully connected layer before the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the layers \n",
    "fashion_model = Sequential()\n",
    "\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(99,13,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))  \n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                      optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the summary\n",
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run. Also here, I just use one epoch for the sake of the example. You should try with at least 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train and test the accuracy in the validation set\n",
    "fashion_train = fashion_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,\n",
    "                                  verbose=1,validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, with this model you should reach 85% accuracy in validation. Not bad eh? However, we can go much more further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to control overfitting by introducing the Dropout function and we hope to improve accuracy. Spoiler alert: we did!\n",
    "\n",
    "To put it simply, during training, some number of layer outputs are randomly ignored or \"dropped out.\" For this we use less parameters and as a result we can control overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters \n",
    "batch_size = 124\n",
    "epochs = 1\n",
    "num_classes = 35 # fix\n",
    "np.random.seed(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up the dropout to improve accuracy\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(99,13,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.25))\n",
    "\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.25))\n",
    "\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.4))\n",
    "\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))  \n",
    "fashion_model.add(Dropout(0.3))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                      optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_train = fashion_model.fit(train_X, train_label, \n",
    "                                  batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                                  validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, with this model you should reach at least 90% accuracy by increasing the number of epochs, let's say a number between 20 and 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althoug, we can reach a good accuracy with 2D layers, the 1D layer turned out to have better accuracy for this data. This is not surprusing, 1D CNN it is widely used for speech recognition. So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data, reshape and transform the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go back to two three dimensions\n",
    "train_X = training_data\n",
    "#test_X = test_data\n",
    "train_Y = labels\n",
    "\n",
    "train_X = train_X.reshape(-1, 99,13)\n",
    "#test_X = test_X.reshape(-1, 99,13)\n",
    "\n",
    "# transform the labels\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "\n",
    "train_X.shape #, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE THE VALIDATION SET \n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, \n",
    "                                                           random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters \n",
    "batch_size = 256\n",
    "epochs = 1\n",
    "num_classes = 35 # fix\n",
    "np.random.seed(222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a very deep model. My group and I spent a lot of time tuning this model because we knew we could reach a very high accuracy. Eventually we were able to reach 95% but I know that it is also possible to get 98%. I leave you this challenge to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model = Sequential()\n",
    "\n",
    "fashion_model.add(Conv1D(64, kernel_size=6,activation='relu',padding='same',input_shape=(99,13)))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "\n",
    "fashion_model.add(Dropout(0.2))\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "fashion_model.add(Conv1D(128, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(256, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(256, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(256, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(512, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(1024, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Conv1D(1024, kernel_size=6, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "fashion_model.add(Dropout(0.2))\n",
    "\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(1024, activation='relu'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))           \n",
    "fashion_model.add(Dropout(0.2))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                      optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_train = fashion_model.fit(train_X, train_label, \n",
    "                                  batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                                  validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I remember correctly, we trained our model for 100 epochs. As always here, for the sake of the example I just ran it for 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have discussed three different ways of dealing with a speech classification task. First, we tried to solve the problem with a ML algorithm, however, we could not go over 20% accuracy, which is normal becuase this kinds of tasks require to apply DL methods. Therefore, two different types of CNN layers were applied: 2D and 1D. As a result a very high accuracy was reached, altoghou a higher one is possible to achive.\n",
    "\n",
    "What do we bring home?\n",
    "- These kinds of tasks cannot be solve with classic ML algortihms\n",
    "- A lot of time is recquired for tuning DL models and unfortunately we do not know which one is the best. Try and error is often the standard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
